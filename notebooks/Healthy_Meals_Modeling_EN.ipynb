{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e60aee98",
   "metadata": {},
   "source": [
    "\n",
    "# Healthy Meals – Modeling (Classification)\n",
    "\n",
    "**Author:** Yosef Reda  \n",
    "**Objective:** Build an English, portfolio‑ready classification model to predict `is_healthy` from meals data, using a reproducible ML pipeline with clean metrics and visuals.\n",
    "\n",
    "**Contents**\n",
    "- Load cleaned data\n",
    "- Train/validation split (stratified)\n",
    "- Preprocessing pipelines (numeric + categorical)\n",
    "- Models: Logistic Regression, Random Forest (with class weighting)\n",
    "- Cross‑validation (ROC AUC)\n",
    "- Evaluation on hold‑out set (ROC, PR, Confusion Matrix, Classification Report)\n",
    "- Feature importance (model‑based & permutation)\n",
    "- Export the best model\n",
    "\n",
    "> **Note**: Educational analysis for this dataset; not medical advice.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf94e085",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import (roc_auc_score, average_precision_score, roc_curve,\n",
    "                             precision_recall_curve, confusion_matrix, classification_report)\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.inspection import permutation_importance\n",
    "import joblib\n",
    "\n",
    "plt.rcParams['figure.dpi'] = 120\n",
    "plt.rcParams['font.size'] = 11\n",
    "plt.rcParams['axes.grid'] = True\n",
    "RANDOM_STATE = 42\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7897103f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "CLEAN_PATH = 'healthy_eating_dataset_clean.csv'\n",
    "if not os.path.exists(CLEAN_PATH):\n",
    "    raise FileNotFoundError(f\"Cleaned file not found: {CLEAN_PATH}. Please run the EDA notebook first.\")\n",
    "\n",
    "df = pd.read_csv(CLEAN_PATH)\n",
    "print(df.shape)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181421a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Drop rows with missing target\n",
    "if 'is_healthy' not in df.columns:\n",
    "    raise KeyError(\"Column 'is_healthy' not found in cleaned data.\")\n",
    "\n",
    "df = df.dropna(subset=['is_healthy']).copy()\n",
    "\n",
    "# Define candidate features (keep it broad, exclude obvious labels/IDs)\n",
    "exclude_cols = {'meal_id','meal_name','image_url','implausible'}\n",
    "features = [c for c in df.columns if c not in exclude_cols.union({'is_healthy'})]\n",
    "\n",
    "# Identify types\n",
    "num_cols = df[features].select_dtypes(include=[np.number]).columns.tolist()\n",
    "cat_cols = [c for c in features if c not in num_cols]\n",
    "\n",
    "X = df[features].copy()\n",
    "y = df['is_healthy'].astype(int)\n",
    "\n",
    "print('Numeric features:', len(num_cols))\n",
    "print('Categorical features:', len(cat_cols))\n",
    "print('Target positive rate:', y.mean().round(3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab5da28",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=RANDOM_STATE\n",
    ")\n",
    "X_train.shape, X_test.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c54f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Numeric: impute median, scale\n",
    "num_pipe = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# Categorical: impute most_frequent, one-hot encode\n",
    "cat_pipe = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('ohe', OneHotEncoder(handle_unknown='ignore', sparse=False))\n",
    "])\n",
    "\n",
    "preprocess = ColumnTransformer([\n",
    "    ('num', num_pipe, num_cols),\n",
    "    ('cat', cat_pipe, cat_cols)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be1ef05",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Models\n",
    "logreg = Pipeline([\n",
    "    ('prep', preprocess),\n",
    "    ('clf', LogisticRegression(max_iter=2000, class_weight='balanced', solver='lbfgs', random_state=RANDOM_STATE))\n",
    "])\n",
    "\n",
    "rf = Pipeline([\n",
    "    ('prep', preprocess),\n",
    "    ('clf', RandomForestClassifier(n_estimators=300, class_weight='balanced', random_state=RANDOM_STATE))\n",
    "])\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\n",
    "\n",
    "# Cross-validated ROC AUC\n",
    "for name, model in [('LogisticRegression', logreg), ('RandomForest', rf)]:\n",
    "    scores = cross_val_score(model, X_train, y_train, cv=cv, scoring='roc_auc', n_jobs=-1)\n",
    "    print(f\"{name} | CV ROC AUC: mean={scores.mean():.3f} ± {scores.std():.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26ac30d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "results = {}\n",
    "\n",
    "for name, model in [('LogisticRegression', logreg), ('RandomForest', rf)]:\n",
    "    model.fit(X_train, y_train)\n",
    "    y_proba = model.predict_proba(X_test)[:,1]\n",
    "    y_pred = (y_proba >= 0.5).astype(int)\n",
    "\n",
    "    roc = roc_auc_score(y_test, y_proba)\n",
    "    prc = average_precision_score(y_test, y_proba)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    rpt = classification_report(y_test, y_pred, digits=3)\n",
    "\n",
    "    results[name] = {'model': model, 'roc_auc': roc, 'ap': prc, 'cm': cm, 'report': rpt, 'y_proba': y_proba, 'y_pred': y_pred}\n",
    "    print(f\"\n",
    "=== {name} ===\n",
    "ROC AUC: {roc:.3f}\n",
    "PR AUC: {prc:.3f}\n",
    "Confusion Matrix:\n",
    "{cm}\n",
    "\n",
    "Report:\n",
    "{rpt}\")\n",
    "\n",
    "# Pick best by ROC AUC\n",
    "best_name = max(results, key=lambda k: results[k]['roc_auc'])\n",
    "best = results[best_name]\n",
    "print(f\"\n",
    "Best model by ROC AUC: {best_name} -> {best['roc_auc']:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a1ae6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "figures = []\n",
    "\n",
    "# ROC curve\n",
    "plt.figure(figsize=(6,4))\n",
    "for name, res in results.items():\n",
    "    fpr, tpr, _ = roc_curve(y_test, res['y_proba'])\n",
    "    plt.plot(fpr, tpr, label=f\"{name} (AUC={roc_auc_score(y_test, res['y_proba']):.2f})\")\n",
    "plt.plot([0,1],[0,1],'k--',alpha=0.5)\n",
    "plt.title('ROC Curve (Hold-out)')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "roc_path = 'en_model_roc_curve.png'\n",
    "plt.savefig(roc_path, dpi=150)\n",
    "figures.append(roc_path)\n",
    "plt.show()\n",
    "\n",
    "# Precision-Recall curve\n",
    "plt.figure(figsize=(6,4))\n",
    "for name, res in results.items():\n",
    "    prec, rec, _ = precision_recall_curve(y_test, res['y_proba'])\n",
    "    plt.plot(rec, prec, label=f\"{name} (AP={average_precision_score(y_test, res['y_proba']):.2f})\")\n",
    "plt.title('Precision-Recall Curve (Hold-out)')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "pr_path = 'en_model_pr_curve.png'\n",
    "plt.savefig(pr_path, dpi=150)\n",
    "figures.append(pr_path)\n",
    "plt.show()\n",
    "\n",
    "# Confusion matrices\n",
    "for name, res in results.items():\n",
    "    cm = res['cm']\n",
    "    plt.figure(figsize=(4,4))\n",
    "    plt.imshow(cm, cmap='Blues')\n",
    "    plt.title(f'Confusion Matrix – {name}')\n",
    "    plt.xticks([0,1], ['Pred 0','Pred 1'])\n",
    "    plt.yticks([0,1], ['True 0','True 1'])\n",
    "    for i in range(2):\n",
    "        for j in range(2):\n",
    "            plt.text(j, i, cm[i,j], ha='center', va='center', color='black')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.tight_layout()\n",
    "    cm_path = f'en_cm_{name.replace(\" \", \"_\")}.png'\n",
    "    plt.savefig(cm_path, dpi=150)\n",
    "    figures.append(cm_path)\n",
    "    plt.show()\n",
    "\n",
    "figures\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f15866",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Helper to get feature names from ColumnTransformer\n",
    "\n",
    "def get_feature_names(preprocessor, num_cols, cat_cols):\n",
    "    names = []\n",
    "    # numeric pipeline columns\n",
    "    names.extend(num_cols)\n",
    "    # categorical OHE names\n",
    "    try:\n",
    "        ohe = preprocessor.named_transformers_['cat'].named_steps['ohe']\n",
    "        cat_names = ohe.get_feature_names_out(cat_cols)\n",
    "        names = num_cols + list(cat_names)\n",
    "    except Exception:\n",
    "        names = num_cols + cat_cols\n",
    "    return names\n",
    "\n",
    "best_pipeline = best['model']\n",
    "prep = best_pipeline.named_steps['prep']\n",
    "feature_names = get_feature_names(prep, num_cols, cat_cols)\n",
    "\n",
    "imp_df = None\n",
    "if 'LogisticRegression' in best_name:\n",
    "    clf = best_pipeline.named_steps['clf']\n",
    "    coefs = clf.coef_.ravel()\n",
    "    imp_df = pd.DataFrame({'feature': feature_names, 'importance': np.abs(coefs)}).sort_values('importance', ascending=False).head(20)\n",
    "elif 'RandomForest' in best_name:\n",
    "    clf = best_pipeline.named_steps['clf']\n",
    "    imps = clf.feature_importances_\n",
    "    imp_df = pd.DataFrame({'feature': feature_names, 'importance': imps}).sort_values('importance', ascending=False).head(20)\n",
    "\n",
    "if imp_df is not None:\n",
    "    plt.figure(figsize=(8,6))\n",
    "    imp_df[::-1].plot(kind='barh', x='feature', y='importance', color='#58D68D', legend=False)\n",
    "    plt.title(f'Top Features – {best_name}')\n",
    "    plt.xlabel('Importance')\n",
    "    plt.tight_layout()\n",
    "    fi_path = 'en_top_features.png'\n",
    "    plt.savefig(fi_path, dpi=150)\n",
    "    plt.show()\n",
    "else:\n",
    "    print('Could not compute model-based feature importance for the best model.')\n",
    "\n",
    "# Permutation importance (on a smaller subset for speed)\n",
    "try:\n",
    "    n_samples = min(1000, X_test.shape[0])\n",
    "    X_sub = X_test.iloc[:n_samples]\n",
    "    y_sub = y_test.iloc[:n_samples]\n",
    "    perm = permutation_importance(best_pipeline, X_sub, y_sub, n_repeats=10, random_state=RANDOM_STATE, scoring='roc_auc')\n",
    "    perm_imp = pd.DataFrame({'feature': feature_names, 'importance_mean': perm.importances_mean})\n",
    "    perm_imp = perm_imp.sort_values('importance_mean', ascending=False).head(20)\n",
    "    plt.figure(figsize=(8,6))\n",
    "    perm_imp[::-1].plot(kind='barh', x='feature', y='importance_mean', color='#AF7AC5', legend=False)\n",
    "    plt.title(f'Permutation Importance – {best_name}')\n",
    "    plt.xlabel('Mean Importance (AUC drop)')\n",
    "    plt.tight_layout()\n",
    "    pfi_path = 'en_perm_importance.png'\n",
    "    plt.savefig(pfi_path, dpi=150)\n",
    "    plt.show()\n",
    "except Exception as e:\n",
    "    print('Permutation importance failed:', e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7cc7777",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "MODEL_PATH = f'best_is_healthy_model_{best_name.replace(\" \", \"_\")}.joblib'\n",
    "joblib.dump(best['model'], MODEL_PATH)\n",
    "MODEL_PATH\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef4c73b",
   "metadata": {},
   "source": [
    "\n",
    "## Summary\n",
    "- Compared **Logistic Regression** and **Random Forest** with 5‑fold stratified CV (ROC AUC).\n",
    "- Evaluated the best model on a hold‑out set with ROC AUC, PR AUC, confusion matrix, and a full classification report.\n",
    "- Reported top features via model‑based importance and **permutation importance**.\n",
    "- Exported the best pipeline as a `.joblib` artifact for reuse in apps/dashboards.\n",
    "\n",
    "> Next: add hyper‑parameter tuning (e.g., `GridSearchCV`), or build a small API/Streamlit demo that loads `joblib` and scores new meals.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
